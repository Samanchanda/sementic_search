 # ğŸ” Semantic Search with HuggingFace Embeddings

This project demonstrates a simple and powerful semantic search system using Hugging Face language models and cosine similarity. It takes a query, encodes it along with a set of documents using transformer-based sentence embeddings, and returns the most semantically similar document.




ğŸš€ Features

Sentence-level semantic understanding using embeddings

Query-to-document matching using cosine similarity

Supports HuggingFace Embeddings via langchain_huggingface

Simple and extensible codebase â€” ideal for educational use and prototyping





ğŸ§  Model Used

Model: BAAI/bge-small-en-v1.5

Lightweight, high-performing embedding model

Provided by Hugging Face and integrated using langchain_huggingface





#ğŸ”§ Technologies

Python

Hugging Face Transformers

LangChain (Embedding layer)

Scikit-learn

NumPy

Google Colab / Jupyter Notebook

dotenv (for managing secrets)





ğŸ“Œ Example

query = "tell me about Peshawar"

Returns the document:

> "Peshawar is an ancient city known for its Pashtun heritage and traditional cuisine."



Similarity Score: 0.87+ (depending on the model version)



ğŸ“ How to Use

1. Install required packages:



pip install langchain-huggingface huggingface_hub python-dotenv sentence-transformers

2. Set your Hugging Face API Token via .env or Colab userdata


3. Run the notebook to test the similarity engine.





ğŸ”— Notebook Link

ğŸ”— View in Google Colab




ğŸ™Œ Contributed by

Saman Tariq
AI Enthusiast | LegalTech Explorer | NLP Practitioner
